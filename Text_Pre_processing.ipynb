{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMNbVod/jm1+8FbzaV3vmM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANIKETRAJ28/NLP_Codes/blob/main/Text_Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jdn4mA9E2rYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Punctuations"
      ],
      "metadata": {
        "id": "AoI0tDK92sU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgYg5w3YCeEK"
      },
      "outputs": [],
      "source": [
        "import string, time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x0Id-pQ1C3Kq",
        "outputId": "f07189a5-bc65-4e29-df1a-977de6b65682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exclude = string.punctuation"
      ],
      "metadata": {
        "id": "xhd6M14FC5iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello there! How are you doing today? I'm fineâ€”thanks for asking. Let's meet at 5:00 p.m., okay?\""
      ],
      "metadata": {
        "id": "VMKxB-TGDHql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(text): # this is very slow because we have built the algo from scratch\n",
        "  for char in exclude:\n",
        "    text = text.replace(char,'')\n",
        "  return text"
      ],
      "metadata": {
        "id": "KBvI17TyDeQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "print(remove_punc(text))\n",
        "time1 = time.time() - start\n",
        "print(time1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzHUxPkqD1-x",
        "outputId": "06961136-73bc-490c-e202-ff20fe743ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there How are you doing today Im fineâ€”thanks for asking Lets meet at 500 pm okay\n",
            "0.0002048015594482422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc1(text):\n",
        "  return text.translate(str.maketrans('', '', exclude))"
      ],
      "metadata": {
        "id": "XrwvBePFEFrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "remove_punc1(text)\n",
        "time2 = time.time() - start\n",
        "print(time1/time2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K0uLg1pGoIp",
        "outputId": "3b58dd27-aa17-47d0-db06-88f487088572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0307328605200947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Correction"
      ],
      "metadata": {
        "id": "3BsaR3k72_CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "DhKcU2g2Gwtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_sentence = \"I definetly recived teh mesage but cudn't undrstand it's purpous.\"\n",
        "textBlg = TextBlob(incorrect_sentence)\n",
        "textBlg.correct().string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CYZTA2ufeU8c",
        "outputId": "7cef4bac-d0be-46b3-eb43-ef64cf1ba836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I definitely received the message but can't understand it's purpose.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stopwords"
      ],
      "metadata": {
        "id": "maIh5Qrv3Dv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrZZkdW4eirv",
        "outputId": "9540e9f4-7b49-42f3-80e6-5e7c4a565de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pFfVzm7qe0YL",
        "outputId": "cd028e95-c9d3-4842-c448-b79bc2c0a9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  new_text = []\n",
        "\n",
        "  for w in text.split():\n",
        "    if w in stopwords.words(\"english\"):\n",
        "      new_text.append('')\n",
        "    else:\n",
        "      new_text.append(w)\n",
        "  # s = new_text[:]\n",
        "  # new_text.clear()\n",
        "  return \" \".join(new_text[:])"
      ],
      "metadata": {
        "id": "LiXj16mQfcZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'The girl walked through the quiet park in the evening, looking at the trees as they swayed in the wind and thinking about the day she had.'\n",
        "remove_stopwords(word)\n",
        "# print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4dVzwg4sg0-V",
        "outputId": "6f870017-f64c-473f-8f5e-1179a0983f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The girl walked   quiet park   evening, looking   trees   swayed   wind  thinking   day  had.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "tgOT2YrH3OP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization via nltk"
      ],
      "metadata": {
        "id": "gBUt82qM3jb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5Eh1dYehUyq",
        "outputId": "d3d68fc7-9a18-4b23-cf6d-2145618d8f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Well, it wasnâ€™t just the state-of-the-art AI-powered toolsâ€”no, it was the usersâ€™ ever-evolving, sometimes unpredictable, behavior that complicated things, wasnâ€™t it?\"\n",
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WUwrbkzgmSuh",
        "outputId": "253b2ff8-60f1-45f2-c840-5de13bfbf8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Well',\n",
              " ',',\n",
              " 'it',\n",
              " 'wasn',\n",
              " 'â€™',\n",
              " 't',\n",
              " 'just',\n",
              " 'the',\n",
              " 'state-of-the-art',\n",
              " 'AI-powered',\n",
              " 'toolsâ€”no',\n",
              " ',',\n",
              " 'it',\n",
              " 'was',\n",
              " 'the',\n",
              " 'users',\n",
              " 'â€™',\n",
              " 'ever-evolving',\n",
              " ',',\n",
              " 'sometimes',\n",
              " 'unpredictable',\n",
              " ',',\n",
              " 'behavior',\n",
              " 'that',\n",
              " 'complicated',\n",
              " 'things',\n",
              " ',',\n",
              " 'wasn',\n",
              " 'â€™',\n",
              " 't',\n",
              " 'it',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Wait...did he just say 'I'm in a@b.com 4pmâ€”lol ðŸ˜‚'?!! That's insane, isn't it?? ðŸ¤¯\"\n",
        "word_tokenize(sentence) # here nltk fails"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RfEt12Wfmcyn",
        "outputId": "c0a178cd-7219-43a2-f810-519b2e1205f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wait',\n",
              " '...',\n",
              " 'did',\n",
              " 'he',\n",
              " 'just',\n",
              " 'say',\n",
              " \"'\",\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'in',\n",
              " 'a',\n",
              " '@',\n",
              " 'b.com',\n",
              " '4pmâ€”lol',\n",
              " 'ðŸ˜‚',\n",
              " \"'\",\n",
              " '?',\n",
              " '!',\n",
              " '!',\n",
              " 'That',\n",
              " \"'s\",\n",
              " 'insane',\n",
              " ',',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'it',\n",
              " '?',\n",
              " '?',\n",
              " 'ðŸ¤¯']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization via spacy"
      ],
      "metadata": {
        "id": "cKzIifCk3oRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "CNEgZ6HonmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentence)"
      ],
      "metadata": {
        "id": "S-XMauJvnzy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMAyjkNEoSeH",
        "outputId": "99e38d93-931f-45da-c980-8cfd105d96ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wait\n",
            "...\n",
            "did\n",
            "he\n",
            "just\n",
            "say\n",
            "'\n",
            "I\n",
            "'m\n",
            "in\n",
            "a@b.com\n",
            "4pm\n",
            "â€”\n",
            "lol\n",
            "ðŸ˜‚\n",
            "'\n",
            "?\n",
            "!\n",
            "!\n",
            "That\n",
            "'s\n",
            "insane\n",
            ",\n",
            "is\n",
            "n't\n",
            "it\n",
            "?\n",
            "?\n",
            "ðŸ¤¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "ChbJlyEX4DLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "5Pk81AE34FhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The children were playing happily while their mothers cooked delicious meals and discussed upcoming holidays.\"\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JKN4zE_V4x9e",
        "outputId": "7c213d5c-063a-471e-9e49-82e7f5a9b1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the children were play happili while their mother cook delici meal and discuss upcom holidays.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "O1Npy52P4_c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download(\"wordnet\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "punctuations = string.punctuation\n",
        "\n",
        "sentence = '''Although the researchers, after months of collecting data, analyzing trends, and reviewing previous studies, remained skeptical; they eventuallyâ€”perhaps reluctantlyâ€”acknowledged that the algorithm's performance, despite its limitations, was improving significantly: faster processing, better accuracy, and reduced errors!'''\n",
        "\n",
        "token_words = word_tokenize(sentence)\n",
        "\n",
        "for word in token_words:\n",
        "  if word in punctuations:\n",
        "    token_words.remove(word)\n",
        "\n",
        "print(token_words)\n",
        "\n",
        "for word in token_words:\n",
        "  print(\"{0:30}{1:30}\".format(word, wordnet_lemmatizer.lemmatize(word), pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThLHjZ1-5BfN",
        "outputId": "79ab4d70-d0af-4b90-e58d-6e795b7a2cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Although', 'the', 'researchers', 'after', 'months', 'of', 'collecting', 'data', 'analyzing', 'trends', 'and', 'reviewing', 'previous', 'studies', 'remained', 'skeptical', 'they', 'eventuallyâ€”perhaps', 'reluctantlyâ€”acknowledged', 'that', 'the', 'algorithm', \"'s\", 'performance', 'despite', 'its', 'limitations', 'was', 'improving', 'significantly', 'faster', 'processing', 'better', 'accuracy', 'and', 'reduced', 'errors']\n",
            "Although                      Although                      \n",
            "the                           the                           \n",
            "researchers                   researcher                    \n",
            "after                         after                         \n",
            "months                        month                         \n",
            "of                            of                            \n",
            "collecting                    collecting                    \n",
            "data                          data                          \n",
            "analyzing                     analyzing                     \n",
            "trends                        trend                         \n",
            "and                           and                           \n",
            "reviewing                     reviewing                     \n",
            "previous                      previous                      \n",
            "studies                       study                         \n",
            "remained                      remained                      \n",
            "skeptical                     skeptical                     \n",
            "they                          they                          \n",
            "eventuallyâ€”perhaps            eventuallyâ€”perhaps            \n",
            "reluctantlyâ€”acknowledged      reluctantlyâ€”acknowledged      \n",
            "that                          that                          \n",
            "the                           the                           \n",
            "algorithm                     algorithm                     \n",
            "'s                            's                            \n",
            "performance                   performance                   \n",
            "despite                       despite                       \n",
            "its                           it                            \n",
            "limitations                   limitation                    \n",
            "was                           wa                            \n",
            "improving                     improving                     \n",
            "significantly                 significantly                 \n",
            "faster                        faster                        \n",
            "processing                    processing                    \n",
            "better                        better                        \n",
            "accuracy                      accuracy                      \n",
            "and                           and                           \n",
            "reduced                       reduced                       \n",
            "errors                        error                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9xc-fqa7ZBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}